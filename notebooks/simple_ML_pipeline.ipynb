{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# DEV302 - Goodbye, deployment headaches: Cloud Deploy and Vertex AI unite\n",
    "\n",
    "{TODO: Update the links below.}\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24743cf4a1e1"
   },
   "source": [
    "**_NOTE_**: This notebook has been tested in the following environment:\n",
    "\n",
    "* Python version = 3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook shows how to run simple Sklearn-based ML pipelines on Vertex AI Pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to build ML pipelines interactivly.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services and resources:\n",
    "\n",
    "- Vertex AI Pipelines\n",
    "- Cloud storage\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Build a data processing component\n",
    "- Build a training component\n",
    "- Build a KFP ML pipeline\n",
    "- Run Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08d289fa873f"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The California housing dataset contains census data of houses found in a given California district in 1990.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aed92deeb4a0"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
    "and [Cloud Storage pricing](https://cloud.google.com/storage/pricing),\n",
    "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install the following packages required to execute this notebook.\n",
    "\n",
    "{TODO: Suggest using the latest major GA version of each package; i.e., --upgrade}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --quiet kfp google-cloud-aiplatform google-cloud-pipeline-components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58707a750154"
   },
   "source": [
    "### Colab only: Uncomment the following cell to restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f200f10a1da3"
   },
   "outputs": [],
   "source": [
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "3. [Enable APIs](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,artifactregistry.googleapis.com).\n",
    "\n",
    "4. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, try the following:\n",
    "* Run `gcloud config list`.\n",
    "* Run `gcloud projects list`.\n",
    "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AsE4Jy5aa99c"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-xLEsto0yp2"
   },
   "source": [
    "### UUID\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a uuid for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7YareiNj00Yw"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 4) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74ccc9e52986"
   },
   "source": [
    "**1. Vertex AI Workbench**\n",
    "* Do nothing as you are already authenticated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de775a3773ba"
   },
   "source": [
    "**2. Local JupyterLab instance, uncomment and run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "254614fa0c46"
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef21552ccea8"
   },
   "source": [
    "**3. Colab, uncomment and run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "603adbbf0532"
   },
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6b2ccc891ed"
   },
   "source": [
    "**4. Service account or other**\n",
    "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "Create a storage bucket to store intermediate artifacts such as datasets.\n",
    "\n",
    "- *{Note to notebook author: For any user-provided strings that need to be unique (like bucket names or model ID's), append \"-unique\" to the end so proper testing can occur}*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account"
   },
   "source": [
    "### Service Account\n",
    "\n",
    "**If you don't know your service account**, try to get your service account using `gcloud` command by executing the second cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_C_BMVpzhug"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_service_account"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    if IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "source": [
    "#### Set service account access for Vertex AI Pipelines\n",
    "\n",
    "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket and register pipeline template in the Artifact Registry that you created in the previous step -- you only need to run these once per service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xy6TrqU2zhug"
   },
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectAdmin {BUCKET_URI}\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f17Vrkqjz710"
   },
   "outputs": [],
   "source": [
    "! gcloud projects add-iam-policy-binding {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=roles/artifactregistry.admin\n",
    "\n",
    "! gcloud projects add-iam-policy-binding {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=roles/artifactregistry.repoAdmin\n",
    "\n",
    "! gcloud projects add-iam-policy-binding {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=roles/artifactregistry.reader\n",
    "\n",
    "! gcloud projects add-iam-policy-binding {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=roles/artifactregistry.reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9a5L7LMp0lmJ"
   },
   "source": [
    "###Â Create a KFP repository in Artifact Registry\n",
    "\n",
    "Create a repository in Artifact Registry for your pipeline templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AK39Nnic1Ode"
   },
   "outputs": [],
   "source": [
    "PIPELINE_TEMPLATE_REPO_NAME = (\n",
    "    f\"your-pipeline-repo-{PROJECT_ID}-unique\"  # @param {type:\"string\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3boHKs-0rUv"
   },
   "outputs": [],
   "source": [
    "! gcloud artifacts repositories create {PIPELINE_TEMPLATE_REPO_NAME} \\\n",
    "    --repository-format=kfp \\\n",
    "    --location={REGION} \\\n",
    "    --description=\"A repository for Vertex AI Pipelines templates\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H09VddAl1tKu"
   },
   "outputs": [],
   "source": [
    "! gcloud artifacts repositories list --project={PROJECT_ID} --location={REGION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "import requests\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components.v1.model import ModelGetOp, ModelUploadOp\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import importer_node\n",
    "from kfp.registry import RegistryClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pt0FnW-b14Iv"
   },
   "source": [
    "### Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAOcaAfI16SQ"
   },
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"california-demo-pipeline\"\n",
    "PIPELINE_ROOT = f\"{BUCKET_URI}/{PIPELINE_NAME}\"\n",
    "MODEL_PATH = f\"{PIPELINE_ROOT}/model\"\n",
    "MODEL_NAME = \"california_reg_model\"\n",
    "PARAM_RUN_1 = {\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"n_estimators\": 4000,\n",
    "    \"max_depth\": 20,\n",
    "    \"random_state\": 8,\n",
    "}\n",
    "\n",
    "PARAM_RUN_2 = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"n_estimators\": 10,\n",
    "    \"max_depth\": 3,\n",
    "    \"random_state\": 8,\n",
    "}\n",
    "\n",
    "DEPLOYED_MODEL_NAME_1 = \"california_reg_model_1\"\n",
    "DEPLOYED_MODEL_NAME_2 = \"california_reg_model_2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0LQAYTia99d"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXawVScmgIZx"
   },
   "source": [
    "### Create pipeline components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqCHg01xgL43"
   },
   "source": [
    "#### Data processing component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ksm8JP9MgRYt"
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"python:3.7\",\n",
    "    packages_to_install=[\"numpy==1.18.5\", \"pandas==1.0.4\", \"scikit-learn==0.23.1\"],\n",
    ")\n",
    "def data_preprocessing_op(processed_dataset: dsl.Output[dsl.Dataset]):\n",
    "\n",
    "    from pathlib import Path as p\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.datasets import fetch_california_housing\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    housing = fetch_california_housing(as_frame=True)\n",
    "    housing_df = housing[\"frame\"]\n",
    "    x_df = housing_df.drop(\"MedHouseVal\", axis=1)\n",
    "    y_df = housing_df[[\"MedHouseVal\"]]\n",
    "    processed_x = SimpleImputer().fit_transform(x_df)\n",
    "    processed_x = StandardScaler().fit_transform(processed_x)\n",
    "\n",
    "    processed_x_df = pd.DataFrame(processed_x, columns=x_df.columns)\n",
    "    housing_df = pd.merge(processed_x_df, y_df, left_index=True, right_index=True)\n",
    "\n",
    "    p(processed_dataset.path).mkdir(exist_ok=True, parents=True)\n",
    "    processed_dataset_path = str(p(processed_dataset.path, \"processed_dataset.csv\"))\n",
    "    housing_df.to_csv(processed_dataset_path, index=False)\n",
    "    processed_dataset.path = processed_dataset_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkim8CvR7m6G"
   },
   "source": [
    "#### Training component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8U04t84v7m6G"
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"python:3.7\",\n",
    "    packages_to_install=[\n",
    "        \"numpy==1.18.5\",\n",
    "        \"pandas==1.0.4\",\n",
    "        \"scikit-learn==0.23.1\",\n",
    "        \"xgboost==1.1.1\",\n",
    "    ],\n",
    ")\n",
    "def training_op(\n",
    "    params: dict,\n",
    "    model_path: str,\n",
    "    processed_dataset: dsl.Input[dsl.Dataset],\n",
    "    trained_model: dsl.Output[dsl.Model],\n",
    "    metrics: dsl.Output[dsl.Metrics],\n",
    "):\n",
    "\n",
    "    from pathlib import Path as p\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from xgboost import XGBRegressor\n",
    "\n",
    "    with open(processed_dataset.path, \"r\") as preprocessed_data:\n",
    "        processed_df = pd.read_csv(preprocessed_data)\n",
    "\n",
    "    x = processed_df.drop(\"MedHouseVal\", axis=1)\n",
    "    y = processed_df[\"MedHouseVal\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        x, y, test_size=0.25, random_state=0\n",
    "    )\n",
    "    model = XGBRegressor()\n",
    "    if params:\n",
    "        model = XGBRegressor(**params)\n",
    "    model = model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = round(np.sqrt(mean_squared_error(y_test, y_pred)), 3)\n",
    "\n",
    "    metrics.log_metric(\"rmse\", rmse)\n",
    "    model_path = model_path.replace(\"gs://\", \"/gcs/\")\n",
    "    p(model_path).mkdir(exist_ok=True, parents=True)\n",
    "    model_filepath = str(p(model_path, \"model.bst\"))\n",
    "    model.save_model(model_filepath)\n",
    "    trained_model.path = model_filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvFIROzNgL1G"
   },
   "source": [
    "### Build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PlX3TM7T1SJF"
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=PIPELINE_NAME,\n",
    ")\n",
    "def pipeline(\n",
    "    params: dict = PARAM_RUN_1, model_path: str = MODEL_PATH, model_name: str = \"None\"\n",
    "):\n",
    "\n",
    "    \"\"\"A demo pipeline.\"\"\"\n",
    "\n",
    "    preprocessing_data_task = data_preprocessing_op()\n",
    "\n",
    "    training_task = training_op(\n",
    "        params=params,\n",
    "        model_path=model_path,\n",
    "        processed_dataset=preprocessing_data_task.outputs[\"processed_dataset\"],\n",
    "    ).after(preprocessing_data_task)\n",
    "\n",
    "    with dsl.If(model_name == \"None\", name=\"champion\"):\n",
    "\n",
    "        model_importer_task = importer_node.importer(\n",
    "            artifact_uri=model_path,\n",
    "            artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "            metadata={\n",
    "                \"containerSpec\": {\n",
    "                    \"imageUri\": \"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-1:latest\"\n",
    "                }\n",
    "            },\n",
    "        ).after(training_task)\n",
    "\n",
    "        model_upload_op = ModelUploadOp(\n",
    "            display_name=MODEL_NAME,\n",
    "            unmanaged_container_model=model_importer_task.outputs[\"artifact\"],\n",
    "            version_aliases=[\"v1\"],\n",
    "            description=\"A simple version of the model\",\n",
    "        ).after(model_importer_task)\n",
    "\n",
    "    with dsl.Else(name=\"challenger\"):\n",
    "\n",
    "        model_importer_task = importer_node.importer(\n",
    "            artifact_uri=model_path,\n",
    "            artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "            metadata={\n",
    "                \"containerSpec\": {\n",
    "                    \"imageUri\": \"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-1:latest\"\n",
    "                }\n",
    "            },\n",
    "        ).after(training_task)\n",
    "\n",
    "        get_model_task = ModelGetOp(model_name=model_name).after(model_importer_task)\n",
    "\n",
    "        model_upload_op = ModelUploadOp(\n",
    "            display_name=MODEL_NAME,\n",
    "            unmanaged_container_model=model_importer_task.outputs[\"artifact\"],\n",
    "            parent_model=get_model_task.outputs[\"model\"],\n",
    "            version_aliases=[\"v2\"],\n",
    "            description=\"A tuned version of the model\",\n",
    "        ).after(get_model_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBgJq3VbxKmJ"
   },
   "source": [
    "### Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZsCqLLG1hED"
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeiYFBcY2DZM"
   },
   "source": [
    "### Upload the pipeline template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Q-R1Xud2P3O"
   },
   "outputs": [],
   "source": [
    "client = RegistryClient(\n",
    "    host=f\"https://{REGION}-kfp.pkg.dev/{PROJECT_ID}/{PIPELINE_TEMPLATE_REPO_NAME}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4UR5Otq2KRV"
   },
   "outputs": [],
   "source": [
    "xgb_pipeline_template, xgb_template_version = client.upload_pipeline(\n",
    "    file_name=\"pipeline.yaml\",\n",
    "    tags=[\"latest\"],\n",
    "    extra_headers={\n",
    "        \"description\": \"This is a Xgboost pipeline template for housing project\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h6LLcmUd3HnB"
   },
   "outputs": [],
   "source": [
    "pipeline_templates = client.list_packages()\n",
    "pipeline_template = client.get_package(package_name=PIPELINE_NAME)\n",
    "print(pipeline_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSeGCoAtgLvg"
   },
   "source": [
    "### Run the pipeline for training the v1 of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LxxK8sh341Mu"
   },
   "outputs": [],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"california-demo-pipeline\",\n",
    "    template_path=f\"https://{REGION}-kfp.pkg.dev/{PROJECT_ID}/{PIPELINE_TEMPLATE_REPO_NAME}/{PIPELINE_NAME}/latest\",\n",
    "    parameter_values={\n",
    "        \"params\": PARAM_RUN_1,\n",
    "        \"model_path\": MODEL_PATH,\n",
    "    },\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJVYBK6OE-CR"
   },
   "source": [
    "### Run the pipeline for training the v2 of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZznChZdyiE8"
   },
   "outputs": [],
   "source": [
    "model_list = aiplatform.Model.list(\n",
    "    filter=f\"display_name={MODEL_NAME}\", order_by=\"create_time\"\n",
    ")\n",
    "model_resource_name = model_list[-1].resource_name\n",
    "model_name = model_list[-1].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GB01p4cWEGN-"
   },
   "outputs": [],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"california-demo-pipeline\",\n",
    "    template_path=f\"https://{REGION}-kfp.pkg.dev/{PROJECT_ID}/{PIPELINE_TEMPLATE_REPO_NAME}/{PIPELINE_NAME}/latest\",\n",
    "    parameter_values={\n",
    "        \"params\": PARAM_RUN_2,\n",
    "        \"model_path\": MODEL_PATH,\n",
    "        \"model_name\": model_name,\n",
    "    },\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eRSvzqbd_M2"
   },
   "source": [
    "### Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0RjU6SC5nc4K"
   },
   "outputs": [],
   "source": [
    "credentials, _ = google.auth.default()\n",
    "authentication = google.auth.transport.requests.Request()\n",
    "credentials.refresh(authentication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0r4zjdOplujt"
   },
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"Authorization\": \"Bearer \" + credentials.token,\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "prediction_data = {\n",
    "    \"instances\": [\n",
    "        [\n",
    "            2.34476576,\n",
    "            0.98214266,\n",
    "            0.62855945,\n",
    "            -0.15375759,\n",
    "            -0.9744286,\n",
    "            -0.04959654,\n",
    "            1.05254828,\n",
    "            -1.32783522,\n",
    "        ]\n",
    "    ]\n",
    "}\n",
    "\n",
    "data = json.dumps(prediction_data).encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yl4xGSRUzq5"
   },
   "source": [
    "##### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGx0DFuLgSkY"
   },
   "outputs": [],
   "source": [
    "model_registry = aiplatform.Model(\n",
    "    model_resource_name\n",
    ").versioning_registry.list_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Ob9NZUue3FO"
   },
   "outputs": [],
   "source": [
    "model_1 = aiplatform.Model(model_registry[0].model_resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3H5jy31q07q"
   },
   "outputs": [],
   "source": [
    "endpoint_1 = aiplatform.Endpoint.create(\n",
    "    display_name=\"endpoint_1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pv5JFGcQeybV"
   },
   "outputs": [],
   "source": [
    "model_1.deploy(\n",
    "    endpoint=endpoint_1,\n",
    "    deployed_model_display_name=DEPLOYED_MODEL_NAME_1,\n",
    "    machine_type=\"n1-standard-4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZZuTGtXpqNk"
   },
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    f\"https://us-central1-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_1.name}:predict\",\n",
    "    headers=headers,\n",
    "    data=data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kyi5wD0TxzhD"
   },
   "outputs": [],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GYStYrvU-Zv"
   },
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tXnMyOUUrW7"
   },
   "outputs": [],
   "source": [
    "model_2 = aiplatform.Model(model_registry[-1].model_resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mbqkU5RUrW-"
   },
   "outputs": [],
   "source": [
    "endpoint_2 = aiplatform.Endpoint.create(\n",
    "    display_name=\"endpoint_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bZu0qlf6UrW-"
   },
   "outputs": [],
   "source": [
    "model_2.deploy(\n",
    "    endpoint=endpoint_2,\n",
    "    deployed_model_display_name=DEPLOYED_MODEL_NAME_2,\n",
    "    machine_type=\"n1-standard-4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dC8W9Wk_UrW_"
   },
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    f\"https://us-central1-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_2.name}:predict\",\n",
    "    headers=headers,\n",
    "    data=data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPCQI39VUrXA"
   },
   "outputs": [],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "\n",
    "delete_pipelines = True\n",
    "if delete_pipelines or os.getenv(\"IS_TESTING\"):\n",
    "    pipelines = aiplatform.PipelineJob.list()\n",
    "    for pipeline in pipelines:\n",
    "        pipeline.delete()\n",
    "\n",
    "delete_endpoints = False\n",
    "if delete_endpoints or os.getenv(\"IS_TESTING\"):\n",
    "    endpoints = aiplatform.Endpoint.list()\n",
    "    for endpoint in endpoints:\n",
    "        endpoint.delete(force=True)\n",
    "\n",
    "delete_models = False\n",
    "if delete_models or os.getenv(\"IS_TESTING\"):\n",
    "    models = aiplatform.Model.list()\n",
    "    for model in models:\n",
    "        model.delete()\n",
    "\n",
    "delete_pipeline_templates = True\n",
    "if delete_pipeline_templates or os.getenv(\"IS_TESTING\"):\n",
    "    pipeline_templates = client.list_packages()\n",
    "    for pipeline_template in pipeline_templates:\n",
    "        _ = client.delete_package(pipeline_template[\"name\"].split(\"/\")[-1])\n",
    "\n",
    "delete_artifact_repo = True\n",
    "if delete_artifact_repo or os.getenv(\"IS_TESTING\"):\n",
    "    ! gcloud artifacts repositories delete {PIPELINE_TEMPLATE_REPO_NAME}\n",
    "\n",
    "delete_bucket = True\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "simple_ML_pipeline.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m118",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu:m118"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
